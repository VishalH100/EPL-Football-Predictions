{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2234e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load training data\n",
    "train_file = 'C:/Users/Vishal/Downloads/EPL Data 2018-2024.xlsx'\n",
    "train_df = pd.read_excel(train_file)\n",
    "\n",
    "# Load test data (for prediction)\n",
    "test_file = 'C:/Users/Vishal/Downloads/testData.xlsx'\n",
    "test_df = pd.read_excel(test_file)\n",
    "\n",
    "# Select input and output columns for training\n",
    "X_train = train_df[['Home_Team', 'Home_xG', 'Away_xG', 'Away_Team', 'Attendance', 'Venue', 'Referee']]\n",
    "y_train = train_df['Score']\n",
    "\n",
    "# Select input columns for test data (for prediction)\n",
    "X_test = test_df[['Home_Team', 'Away_Team', 'Attendance', 'Venue', 'Referee']]\n",
    "\n",
    "# Impute missing values in training and test data\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train[['Home_xG', 'Away_xG', 'Attendance']] = imputer.fit_transform(X_train[['Home_xG', 'Away_xG', 'Attendance']])\n",
    "X_test[['Home_xG', 'Away_xG', 'Attendance']] = imputer.transform(X_test[['Home_xG', 'Away_xG', 'Attendance']])\n",
    "\n",
    "# Concatenate training and test data\n",
    "combined_data = pd.concat([X_train, X_test])\n",
    "\n",
    "# Convert categorical variables to dummy variables for combined data\n",
    "combined_data = pd.get_dummies(combined_data)\n",
    "\n",
    "# Split back into training and test data\n",
    "X_train_encoded = combined_data[:len(X_train)]\n",
    "X_test_encoded = combined_data[len(X_train):]\n",
    "\n",
    "# Ensure columns present in both training and test data after one-hot encoding\n",
    "common_columns = set(X_train_encoded.columns) & set(X_test_encoded.columns)\n",
    "X_train_encoded = X_train_encoded[common_columns]\n",
    "X_test_encoded = X_test_encoded[common_columns]\n",
    "\n",
    "# Now, impute missing values in the test data\n",
    "X_test_encoded[['Home_xG', 'Away_xG', 'Attendance']] = imputer.transform(X_test_encoded[['Home_xG', 'Away_xG', 'Attendance']])\n",
    "\n",
    "# Splitting back doesn't preserve index, resetting it\n",
    "X_train_encoded.reset_index(drop=True, inplace=True)\n",
    "X_test_encoded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = model.predict(X_test_encoded)\n",
    "\n",
    "# Display predictions for test data\n",
    "test_df['Predicted_Score'] = y_test_pred\n",
    "print(test_df[['Home_Team', 'Away_Team', 'Attendance', 'Venue', 'Referee', 'Predicted_Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c140665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "    # Extracting scores\n",
    "    data['Home_Score'] = data['Score'].str.split('—', expand=True)[0].astype(int)\n",
    "    data['Away_Score'] = data['Score'].str.split('—', expand=True)[1].astype(int)\n",
    "\n",
    "    # Encoding teams\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    teams = data['Home_Team'].append(data['Away_Team']).unique().reshape(-1, 1)\n",
    "    encoder.fit(teams)\n",
    "\n",
    "    home_teams_encoded = encoder.transform(data[['Home_Team']])\n",
    "    away_teams_encoded = encoder.transform(data[['Away_Team']])\n",
    "\n",
    "    data = pd.concat([\n",
    "        data,\n",
    "        pd.DataFrame(home_teams_encoded, columns=[f\"home_{team}\" for team in encoder.categories_[0]]),\n",
    "        pd.DataFrame(away_teams_encoded, columns=[f\"away_{team}\" for team in encoder.categories_[0]])\n",
    "    ], axis=1)\n",
    "\n",
    "    # Scaling scores\n",
    "    scaler = MinMaxScaler()\n",
    "    data[['Home_Score', 'Away_Score']] = scaler.fit_transform(data[['Home_Score', 'Away_Score']])\n",
    "\n",
    "    return data, scaler, encoder\n",
    "\n",
    "def create_sequences(data, n_steps=3):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data.iloc[i:(i+n_steps)].drop(['Score', 'Home_Team', 'Away_Team', 'Date', 'Home_Score', 'Away_Score'], axis=1).values)\n",
    "        y.append(data.iloc[i + n_steps][['Home_Score', 'Away_Score']])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        Dense(2)  # Predicting two scores: home and away\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Assuming the CSV file is named 'soccer_data.csv' and is located in the specified directory\n",
    "    filepath = 'C:/Users/Vishal/Desktop/Dissertation/Datasets/EPL Data 2018-2024.csv'\n",
    "    data, scaler, encoder = load_and_preprocess_data(filepath)\n",
    "\n",
    "    # Prepare the sequences\n",
    "    X, y = create_sequences(data)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build the model\n",
    "    model = build_model((X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Save the model and scaler for later use\n",
    "    model.save('football_score_predictor_model.h5')\n",
    "    # Assume you handle scaler saving and loading yourself\n",
    "\n",
    "    print(\"Model training complete and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71cbc7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 3s 8ms/step - loss: 5.5247e-05\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 9.2255e-06\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.9255e-06\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.7222e-06\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 9.5426e-07\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 6.4201e-07\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 4.3963e-07\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 3.2881e-07\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 2.4032e-07\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.7527e-07\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4527e-07\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1727e-07\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 9.9401e-08\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 8.9828e-08\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 7.6437e-08\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 6.7788e-08\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 6.1794e-08\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 6.2793e-08\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 5.0716e-08\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 4.2884e-08\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 4.1076e-08\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.9827e-08\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7469e-08\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 3.4497e-08\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.0030e-08\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.7098e-08\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 2.9258e-08\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.5873e-08\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.3154e-08\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.0092e-08\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.2744e-08\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9291e-08\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8988e-08\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9871e-08\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8570e-08\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8070e-08\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9015e-08\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.7900e-08\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5825e-08\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5738e-08\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.6912e-08\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8907e-08\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.2136e-08\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.5990e-08\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.5432e-08\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.2743e-08\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2476e-08\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.6188e-08\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3093e-08\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2175e-08\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_season_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m     predictions_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Vishal/Desktop/Dissertation/Datasets/EPL Data 2018-2024/Results/2023-2024_predictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 71\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[18], line 67\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Here you would handle predictions and save to CSV\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# This part needs the data for the season 2023-2024 which should be processed similarly\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(new_season_data)\n\u001b[0;32m     68\u001b[0m predictions_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Vishal/Desktop/Dissertation/Datasets/EPL Data 2018-2024/Results/2023-2024_predictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_season_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from datetime import datetime\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    try:\n",
    "        data = pd.read_csv(filepath, encoding='ISO-8859-1')\n",
    "    except UnicodeDecodeError:\n",
    "        data = pd.read_csv(filepath, encoding='utf-8')\n",
    "\n",
    "    # Attempt to extract scores and handle cases where extraction fails\n",
    "    scores = data['Score'].str.extract('(\\d+)[—-](\\d+)')\n",
    "    data['home_team_score'], data['away_team_score'] = scores[0].fillna(-1).astype(int), scores[1].fillna(-1).astype(int)\n",
    "\n",
    "    # Convert Date to datetime\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y')\n",
    "\n",
    "    # Filter for seasons from 2018-2019 to 2022-2023\n",
    "    data = data[data['Season'].isin(['2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023'])]\n",
    "\n",
    "    # Encode teams\n",
    "    encoder = LabelEncoder()\n",
    "    data['Home_Team'] = encoder.fit_transform(data['Home_Team'])\n",
    "    data['Away_Team'] = encoder.transform(data['Away_Team'])\n",
    "    \n",
    "    return data, encoder\n",
    "\n",
    "def prepare_inputs(data):\n",
    "    n_input_steps = 3  # Number of past records to consider\n",
    "    n_features = 4     # home_team, away_team, home_team_score, away_team_score\n",
    "\n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(data[['Home_Team', 'Away_Team', 'home_team_score', 'away_team_score']])\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(n_input_steps, len(data_scaled)):\n",
    "        X.append(data_scaled[i-n_input_steps:i, :])\n",
    "        y.append(data_scaled[i, 2:4])  # Indices for scores\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model(n_input_steps, n_features):\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(n_input_steps, n_features)),\n",
    "        Dropout(0.5),\n",
    "        LSTM(50),\n",
    "        Dense(2)  # Predicting two scores: home and away\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    filepath = 'C:/Users/Vishal/Desktop/Dissertation/Datasets/EPL Data 2018-2024.csv'\n",
    "    data, team_encoder = load_and_preprocess_data(filepath)\n",
    "    \n",
    "    X, y = prepare_inputs(data)\n",
    "    \n",
    "    model = build_model(X.shape[1], X.shape[2])  # Pass the shape of the input\n",
    "    model.fit(X, y, epochs=50, batch_size=64)\n",
    "    \n",
    "    # Here you would handle predictions and save to CSV\n",
    "    # This part needs the data for the season 2023-2024 which should be processed similarly\n",
    "#     predictions = model.predict(new_season_data)\n",
    "#     predictions_df.to_csv('C:/Users/Vishal/Desktop/Dissertation/Datasets/EPL Data 2018-2024/Results/2023-2024_predictions.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "732245df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60/60 [==============================] - 3s 8ms/step - loss: 1.6787e-07\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "12/12 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from datetime import datetime\n",
    "\n",
    "def load_and_preprocess_data(filepath, encoder=None, scaler=None, is_train=True):\n",
    "    data = pd.read_csv(filepath, encoding='ISO-8859-1')\n",
    "\n",
    "    # Extract scores and handle cases where extraction fails\n",
    "    scores = data['Score'].str.extract('(\\d+)[—-](\\d+)')\n",
    "    data['home_team_score'], data['away_team_score'] = scores[0].fillna(-1).astype(int), scores[1].fillna(-1).astype(int)\n",
    "\n",
    "    # Convert Date to datetime\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y')\n",
    "\n",
    "    # Filter by season if training data\n",
    "    if is_train:\n",
    "        data = data[data['Season'].isin(['2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023'])]\n",
    "\n",
    "    # Get all unique team names from the dataset\n",
    "    if is_train and not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        all_teams = pd.concat([data['Home_Team'], data['Away_Team']]).unique()\n",
    "        all_teams = np.append(all_teams, \"Unknown\")  # Append 'Unknown' for unseen teams\n",
    "        encoder.fit(all_teams)\n",
    "    \n",
    "    # Transform team names with handling for unseen teams\n",
    "    data['Home_Team'] = data['Home_Team'].apply(lambda x: x if x in encoder.classes_ else \"Unknown\")\n",
    "    data['Away_Team'] = data['Away_Team'].apply(lambda x: x if x in encoder.classes_ else \"Unknown\")\n",
    "    data['Home_Team'] = encoder.transform(data['Home_Team'])\n",
    "    data['Away_Team'] = encoder.transform(data['Away_Team'])\n",
    "\n",
    "    # Scaling features\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        data_scaled = scaler.fit_transform(data[['Home_Team', 'Away_Team', 'home_team_score', 'away_team_score']])\n",
    "    else:\n",
    "        data_scaled = scaler.transform(data[['Home_Team', 'Away_Team', 'home_team_score', 'away_team_score']])\n",
    "    \n",
    "    return data, data_scaled, encoder, scaler\n",
    "\n",
    "\n",
    "def build_model(n_input_steps, n_features):\n",
    "    model = Sequential([\n",
    "        LSTM(100, return_sequences=True, input_shape=(n_input_steps, n_features)),  # Increased complexity\n",
    "        Dropout(0.3),  # Adjusted dropout rate\n",
    "        LSTM(100),\n",
    "        Dense(2, activation='relu')  # Ensures non-negative outputs directly\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    train_filepath = 'C:/Users/Vishal/Desktop/Dissertation/Datasets/EPL Data 2018-2024.csv'\n",
    "    test_filepath = 'C:/Users/Vishal/Desktop/Dissertation/EPL Data 2024.csv'\n",
    "    \n",
    "    # Load and preprocess training data\n",
    "    train_data, train_scaled, encoder, scaler = load_and_preprocess_data(train_filepath)\n",
    "\n",
    "    # Prepare training inputs\n",
    "    X, y = prepare_inputs(train_scaled, 3)\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = build_model(X.shape[1], X.shape[2])\n",
    "    model.fit(X, y, epochs=100, batch_size=32)  # Adjusted epochs and batch size\n",
    "\n",
    "    # Load and preprocess test data for the 2023-2024 season\n",
    "    test_data, test_scaled, _, _ = load_and_preprocess_data(test_filepath, encoder, scaler, is_train=False)\n",
    "    \n",
    "    # Prepare test inputs\n",
    "    X_test, _ = prepare_inputs(test_scaled, 3)\n",
    "\n",
    "    # Predict for the 2023-2024 season\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Post-process predictions: round to nearest integer (model already ensures non-negative via ReLU)\n",
    "    predictions = np.round(predictions)\n",
    "\n",
    "    # Combine predictions with the original test data\n",
    "    predictions_df = test_data.iloc[3:].copy()  # Adjust index based on window size\n",
    "    predictions_df['Predicted_Home_Score'], predictions_df['Predicted_Away_Score'] = predictions[:, 0], predictions[:, 1]\n",
    "    \n",
    "    predictions_df.to_csv('C:/Users/Vishal/Desktop/Dissertation/Datasets/Results/2023-2024_predictions.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb2834ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60/60 [==============================] - 4s 9ms/step - loss: 3.4193e-07\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.0000e+00\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0000e+00\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0000e+00\n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0000e+00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'Newcastle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Newcastle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m     test_data_adjusted\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Vishal/Desktop/Dissertation/Datasets/Results/2023-2024_predictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 79\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[34], line 67\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m test_data, test_scaled, _, _ \u001b[38;5;241m=\u001b[39m load_and_preprocess_data(test_filepath, encoder, scaler, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m X_test, _ \u001b[38;5;241m=\u001b[39m prepare_inputs(test_scaled, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     69\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[34], line 30\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[1;34m(filepath, encoder, scaler, is_train)\u001b[0m\n\u001b[0;32m     27\u001b[0m     all_teams \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHome_Team\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAway_Team\u001b[39m\u001b[38;5;124m'\u001b[39m]])\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     28\u001b[0m     encoder\u001b[38;5;241m.\u001b[39mfit(all_teams)\n\u001b[1;32m---> 30\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHome_Team\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHome_Team\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     31\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAway_Team\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAway_Team\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scaler:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 'Newcastle'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_scores(score_str):\n",
    "    if not isinstance(score_str, str):\n",
    "        return -1, -1  # Handle cases where the score might be NaN or similar\n",
    "    for delimiter in ['—', '-', '–']:\n",
    "        if delimiter in score_str:\n",
    "            parts = score_str.split(delimiter)\n",
    "            return int(parts[0]), int(parts[1])\n",
    "    return -1, -1\n",
    "\n",
    "def load_and_preprocess_data(filepath, encoder=None, scaler=None, is_train=True):\n",
    "    data = pd.read_csv(filepath, encoding='ISO-8859-1')\n",
    "    data[['home_team_score', 'away_team_score']] = pd.DataFrame(data['Score'].apply(parse_scores).tolist(), index=data.index)\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y')\n",
    "\n",
    "    if is_train:\n",
    "        data = data[data['Season'].isin(['2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023'])]\n",
    "\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        all_teams = pd.concat([data['Home_Team'], data['Away_Team']]).unique()\n",
    "        encoder.fit(all_teams)\n",
    "    \n",
    "    data['Home_Team'] = encoder.transform(data['Home_Team'])\n",
    "    data['Away_Team'] = encoder.transform(data['Away_Team'])\n",
    "\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        data_scaled = scaler.fit_transform(data[['Home_Team', 'Away_Team', 'home_team_score', 'away_team_score']])\n",
    "    else:\n",
    "        data_scaled = scaler.transform(data[['Home_Team', 'Away_Team', 'home_team_score', 'away_team_score']])\n",
    "    \n",
    "    return data, data_scaled, encoder, scaler\n",
    "\n",
    "def prepare_inputs(data_scaled, n_input_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(n_input_steps, len(data_scaled)):\n",
    "        X.append(data_scaled[i-n_input_steps:i, :])\n",
    "        y.append(data_scaled[i, 2:4])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model(n_input_steps, n_features):\n",
    "    model = Sequential([\n",
    "        LSTM(100, return_sequences=True, input_shape=(n_input_steps, n_features)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(100),\n",
    "        Dense(2, activation='relu')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    train_filepath = 'C:/Users/Vishal/Desktop/Dissertation/EPL Data 2018-2024.csv'\n",
    "    test_filepath = 'C:/Users/Vishal/Desktop/Dissertation/EPL Data 2024.csv'\n",
    "    \n",
    "    train_data, train_scaled, encoder, scaler = load_and_preprocess_data(train_filepath)\n",
    "    X, y = prepare_inputs(train_scaled, 3)\n",
    "    model = build_model(X.shape[1], X.shape[2])\n",
    "    model.fit(X, y, epochs=100, batch_size=32)\n",
    "\n",
    "    test_data, test_scaled, _, _ = load_and_preprocess_data(test_filepath, encoder, scaler, is_train=False)\n",
    "    X_test, _ = prepare_inputs(test_scaled, 3)\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = np.round(predictions)\n",
    "\n",
    "    test_data_adjusted = test_data.iloc[3:]\n",
    "    test_data_adjusted['Predicted_Home_Score'] = predictions[:, 0]\n",
    "    test_data_adjusted['Predicted_Away_Score'] = predictions[:, 1]\n",
    "\n",
    "    test_data_adjusted.to_csv('C:/Users/Vishal/Desktop/Dissertation/Datasets/Results/2023-2024_predictions.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd9c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96f547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
